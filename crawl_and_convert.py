#!/usr/bin/env python3
"""
SEC-API æ–‡æ¡£ç«™ç‚¹åˆ†æå’Œè½¬æ¢å·¥å…·
åŠŸèƒ½ï¼š
1. åˆ†æ wget ä¸‹è½½çš„ç«™ç‚¹ç»“æ„
2. æå–æ‰€æœ‰é“¾æ¥
3. å°† HTML è½¬æ¢ä¸º Markdown
4. ç”Ÿæˆç«™ç‚¹åœ°å›¾å’Œåˆ†ææŠ¥å‘Š
"""

import os
import json
import re
from pathlib import Path
from urllib.parse import urljoin, urlparse, unquote
from collections import defaultdict
from datetime import datetime

from bs4 import BeautifulSoup
from markitdown import MarkItDown
import html2text


class SiteAnalyzer:
    def __init__(self, download_dir, output_dir="output"):
        self.download_dir = Path(download_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # åˆå§‹åŒ–è½¬æ¢å™¨
        self.markitdown = MarkItDown()
        self.html2text = html2text.HTML2Text()
        self.html2text.ignore_links = False
        self.html2text.ignore_images = False

        # æ•°æ®å­˜å‚¨
        self.all_files = []
        self.all_links = defaultdict(list)
        self.internal_links = set()
        self.external_links = set()
        self.broken_links = []
        self.file_structure = {}

    def scan_files(self):
        """æ‰«ææ‰€æœ‰ HTML æ–‡ä»¶"""
        print("ğŸ“ æ‰«ææ–‡ä»¶...")
        for file_path in self.download_dir.rglob("*.html"):
            self.all_files.append(file_path)

        print(f"âœ… æ‰¾åˆ° {len(self.all_files)} ä¸ª HTML æ–‡ä»¶")
        return self.all_files

    def extract_links(self):
        """ä»æ‰€æœ‰ HTML æ–‡ä»¶ä¸­æå–é“¾æ¥"""
        print("\nğŸ”— æå–é“¾æ¥...")

        for file_path in self.all_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    soup = BeautifulSoup(f.read(), 'html.parser')

                # æå–é¡µé¢æ ‡é¢˜
                title = soup.find('title')
                title_text = title.text.strip() if title else "æ— æ ‡é¢˜"

                # è·å–ç›¸å¯¹è·¯å¾„
                relative_path = file_path.relative_to(self.download_dir)

                # æå–æ‰€æœ‰é“¾æ¥
                file_links = []
                for tag in soup.find_all(['a', 'link']):
                    href = tag.get('href')
                    if href:
                        file_links.append({
                            'href': href,
                            'text': tag.text.strip() if tag.name == 'a' else '',
                            'tag': tag.name
                        })

                        # åˆ†ç±»é“¾æ¥
                        if href.startswith(('http://', 'https://')):
                            if 'sec-api.io' in href:
                                self.internal_links.add(href)
                            else:
                                self.external_links.add(href)
                        elif href.startswith('/'):
                            self.internal_links.add(
                                f"https://sec-api.io{href}")

                self.all_links[str(relative_path)] = {
                    'title': title_text,
                    'links': file_links,
                    'path': str(file_path)
                }

            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        print(
            f"âœ… æå–å®Œæˆ: {len(self.internal_links)} ä¸ªå†…éƒ¨é“¾æ¥, {len(self.external_links)} ä¸ªå¤–éƒ¨é“¾æ¥")

    def check_broken_links(self):
        """æ£€æŸ¥æ­»é“¾æ¥"""
        print("\nğŸ” æ£€æŸ¥æ­»é“¾æ¥...")

        for file_info in self.all_links.values():
            file_path = Path(file_info['path'])
            file_dir = file_path.parent

            for link_info in file_info['links']:
                href = link_info['href']

                # åªæ£€æŸ¥ç›¸å¯¹é“¾æ¥
                if not href.startswith(('http://', 'https://', '#', 'mailto:', 'javascript:')):
                    # è§£æç›¸å¯¹è·¯å¾„
                    if href.startswith('/'):
                        # ç»å¯¹è·¯å¾„
                        target_path = self.download_dir / \
                            'sec-api.io' / href.lstrip('/')
                    else:
                        # ç›¸å¯¹è·¯å¾„
                        target_path = (file_dir / href).resolve()

                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    exists = (target_path.exists() or
                              target_path.with_suffix('.html').exists() or
                              (target_path / 'index.html').exists())

                    if not exists:
                        self.broken_links.append({
                            'source': str(file_path.relative_to(self.download_dir)),
                            'broken_link': href,
                            'expected_path': str(target_path.relative_to(self.download_dir))
                        })

        print(f"âœ… å‘ç° {len(self.broken_links)} ä¸ªæ­»é“¾æ¥")

    def convert_to_markdown(self):
        """å°†æ‰€æœ‰ HTML æ–‡ä»¶è½¬æ¢ä¸º Markdown"""
        print("\nğŸ“ è½¬æ¢ä¸º Markdown...")

        markdown_dir = self.output_dir / 'markdown'
        markdown_dir.mkdir(exist_ok=True)

        converted_count = 0
        failed_count = 0

        for file_path in self.all_files:
            try:
                # è¯»å– HTML
                with open(file_path, 'r', encoding='utf-8') as f:
                    html_content = f.read()

                soup = BeautifulSoup(html_content, 'html.parser')

                # è·å–æ ‡é¢˜
                title = soup.find('title')
                title_text = title.text.strip() if title else "Documentation"

                # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
                content = None
                for selector in ['main', 'article', '.content', '#content', '.docs-content']:
                    content = soup.select_one(selector)
                    if content:
                        break

                if not content:
                    content = soup.body if soup.body else soup

                # å°è¯•ä½¿ç”¨ markitdown
                try:
                    result = self.markitdown.convert(str(content))
                    markdown_content = result.text_content
                except:
                    # é™çº§ä½¿ç”¨ html2text
                    markdown_content = self.html2text.handle(str(content))

                # æ„å»ºè¾“å‡ºè·¯å¾„
                relative_path = file_path.relative_to(self.download_dir)
                output_path = markdown_dir / relative_path.with_suffix('.md')
                output_path.parent.mkdir(parents=True, exist_ok=True)

                # å†™å…¥ Markdown æ–‡ä»¶
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(f"# {title_text}\n\n")
                    f.write(f"> æºæ–‡ä»¶: {relative_path}\n")
                    f.write(
                        f"> è½¬æ¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                    f.write("---\n\n")
                    f.write(markdown_content)

                converted_count += 1

            except Exception as e:
                print(f"âŒ è½¬æ¢å¤±è´¥ {file_path}: {e}")
                failed_count += 1

        print(f"âœ… è½¬æ¢å®Œæˆ: æˆåŠŸ {converted_count} ä¸ª, å¤±è´¥ {failed_count} ä¸ª")

    def generate_sitemap(self):
        """ç”Ÿæˆç«™ç‚¹åœ°å›¾"""
        print("\nğŸ—ºï¸  ç”Ÿæˆç«™ç‚¹åœ°å›¾...")

        sitemap = {
            'pages': [],
            'structure': {},
            'statistics': {}
        }

        # æ„å»ºé¡µé¢åˆ—è¡¨
        for file_path in self.all_files:
            relative_path = file_path.relative_to(self.download_dir)

            # è½¬æ¢ä¸º URL
            url_parts = str(relative_path).replace('\\', '/').split('/')
            if url_parts[0] == 'sec-api.io':
                url_parts = url_parts[1:]

            url_path = '/'.join(url_parts)
            if url_path.endswith('/index.html'):
                url_path = url_path[:-11]
            elif url_path.endswith('.html'):
                url_path = url_path[:-5]

            # è·å–é¡µé¢ä¿¡æ¯
            file_key = str(relative_path)
            page_info = {
                'url': f"https://sec-api.io/{url_path}",
                'file': str(relative_path),
                'title': self.all_links.get(file_key, {}).get('title', ''),
                'links_count': len(self.all_links.get(file_key, {}).get('links', []))
            }

            sitemap['pages'].append(page_info)

            # æ„å»ºæ ‘å½¢ç»“æ„
            current = sitemap['structure']
            for part in url_parts[:-1]:
                if part not in current:
                    current[part] = {}
                current = current[part]

            file_name = url_parts[-1] if url_parts else 'index'
            current[file_name] = page_info['url']

        # ç»Ÿè®¡ä¿¡æ¯
        sitemap['statistics'] = {
            'total_pages': len(self.all_files),
            'total_internal_links': len(self.internal_links),
            'total_external_links': len(self.external_links),
            'broken_links': len(self.broken_links),
            'generated_at': datetime.now().isoformat()
        }

        # ä¿å­˜ç«™ç‚¹åœ°å›¾
        with open(self.output_dir / 'sitemap.json', 'w', encoding='utf-8') as f:
            json.dump(sitemap, f, indent=2, ensure_ascii=False)

        print("âœ… ç«™ç‚¹åœ°å›¾å·²ç”Ÿæˆ")

    def generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆåˆ†ææŠ¥å‘Š...")

        report = []
        report.append("# SEC-API æ–‡æ¡£ç«™ç‚¹åˆ†ææŠ¥å‘Š\n")
        report.append(
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        # åŸºæœ¬ç»Ÿè®¡
        report.append("## ğŸ“ˆ åŸºæœ¬ç»Ÿè®¡\n")
        report.append(f"- æ€»é¡µé¢æ•°: {len(self.all_files)}")
        report.append(f"- å†…éƒ¨é“¾æ¥: {len(self.internal_links)}")
        report.append(f"- å¤–éƒ¨é“¾æ¥: {len(self.external_links)}")
        report.append(f"- æ­»é“¾æ¥: {len(self.broken_links)}\n")

        # ç›®å½•ç»“æ„
        report.append("## ğŸ“ ç›®å½•ç»“æ„\n")
        report.append("```")
        for file_path in sorted(self.all_files)[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
            relative_path = file_path.relative_to(self.download_dir)
            report.append(f"  {relative_path}")
        if len(self.all_files) > 20:
            report.append(f"  ... è¿˜æœ‰ {len(self.all_files) - 20} ä¸ªæ–‡ä»¶")
        report.append("```\n")

        # æ­»é“¾æ¥æŠ¥å‘Š
        if self.broken_links:
            report.append("## âŒ æ­»é“¾æ¥åˆ—è¡¨\n")
            for broken in self.broken_links[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                report.append(f"- **{broken['source']}**")
                report.append(f"  - é“¾æ¥: `{broken['broken_link']}`")
                report.append(f"  - æœŸæœ›è·¯å¾„: `{broken['expected_path']}`\n")

            if len(self.broken_links) > 10:
                report.append(f"... è¿˜æœ‰ {len(self.broken_links) - 10} ä¸ªæ­»é“¾æ¥\n")

        # å¤–éƒ¨é“¾æ¥
        report.append("## ğŸŒ å¤–éƒ¨é“¾æ¥åŸŸåç»Ÿè®¡\n")
        external_domains = defaultdict(int)
        for link in self.external_links:
            domain = urlparse(link).netloc
            external_domains[domain] += 1

        for domain, count in sorted(external_domains.items(), key=lambda x: x[1], reverse=True)[:10]:
            report.append(f"- {domain}: {count} ä¸ªé“¾æ¥")

        # ä¿å­˜æŠ¥å‘Š
        report_path = self.output_dir / 'analysis_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))

        print("âœ… æŠ¥å‘Šå·²ç”Ÿæˆ")

        # åŒæ—¶ä¿å­˜è¯¦ç»†çš„ JSON æ•°æ®
        detailed_data = {
            'all_links': dict(self.all_links),
            'internal_links': list(self.internal_links),
            'external_links': list(self.external_links),
            'broken_links': self.broken_links,
            'file_list': [str(f.relative_to(self.download_dir)) for f in self.all_files]
        }

        with open(self.output_dir / 'detailed_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(detailed_data, f, indent=2, ensure_ascii=False)

    def run(self):
        """è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹"""
        print("ğŸš€ å¼€å§‹åˆ†æ SEC-API æ–‡æ¡£ç«™ç‚¹\n")

        # 1. æ‰«ææ–‡ä»¶
        self.scan_files()

        # 2. æå–é“¾æ¥
        self.extract_links()

        # 3. æ£€æŸ¥æ­»é“¾æ¥
        self.check_broken_links()

        # 4. è½¬æ¢ä¸º Markdown
        self.convert_to_markdown()

        # 5. ç”Ÿæˆç«™ç‚¹åœ°å›¾
        self.generate_sitemap()

        # 6. ç”ŸæˆæŠ¥å‘Š
        self.generate_report()

        print("\nâœ¨ åˆ†æå®Œæˆï¼")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.output_dir.absolute()}")
        print("ğŸ“„ æ–‡ä»¶åˆ—è¡¨:")
        print(f"  - markdown/: è½¬æ¢åçš„ Markdown æ–‡ä»¶")
        print(f"  - sitemap.json: ç«™ç‚¹åœ°å›¾")
        print(f"  - analysis_report.md: åˆ†ææŠ¥å‘Š")
        print(f"  - detailed_analysis.json: è¯¦ç»†æ•°æ®")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='åˆ†æ wget ä¸‹è½½çš„ç«™ç‚¹å¹¶è½¬æ¢ä¸º Markdown')
    parser.add_argument('download_dir', help='wget ä¸‹è½½çš„ç›®å½•è·¯å¾„')
    parser.add_argument('-o', '--output', default='output',
                        help='è¾“å‡ºç›®å½• (é»˜è®¤: output)')

    args = parser.parse_args()

    # æ£€æŸ¥ä¸‹è½½ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.download_dir):
        print(f"âŒ é”™è¯¯: ç›®å½• '{args.download_dir}' ä¸å­˜åœ¨")
        print("\nè¯·å…ˆä½¿ç”¨ wget ä¸‹è½½ç«™ç‚¹:")
        print("wget --mirror --convert-links --page-requisites --no-parent \\")
        print("     --directory-prefix=sec-api-docs \\")
        print("     https://sec-api.io/docs")
        return

    # è¿è¡Œåˆ†æ
    analyzer = SiteAnalyzer(args.download_dir, args.output)
    analyzer.run()


if __name__ == "__main__":
    main()
